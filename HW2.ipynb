{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41c8e572-9ceb-4153-8695-f67a9938ecc6",
   "metadata": {},
   "source": [
    "# HW 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a46530f-25fa-4e6f-8796-0c6c852ee674",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1a79511-28e9-4419-8061-5da719c99833",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.preprocessing import MinMaxScaler,StandardScaler,QuantileTransformer\n",
    "from scipy.spatial.distance import cdist\n",
    "from sklearn import linear_model, datasets\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c812e073-88f9-4b26-84b8-fe7908f6e221",
   "metadata": {},
   "outputs": [],
   "source": [
    "import Lowess\n",
    "import Logistic as LowessLR\n",
    "import g_boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d1690bd-ff35-434b-9829-66aef4102f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install xgboost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da631247-68aa-43c2-b118-6b36c17f0c8d",
   "metadata": {},
   "source": [
    "#### Kernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de8bf85f-6e9c-4163-80a8-f0e096f20bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gaussian Kernel\n",
    "def Gaussian(x):\n",
    "  return np.where(np.abs(x)>4,0,1/(np.sqrt(2*np.pi))*np.exp(-1/2*x**2))\n",
    "    \n",
    "# Tricubic Kernel\n",
    "def Tricubic(x):\n",
    "  return np.where(np.abs(x)>1,0,(1-np.abs(x)**3)**3)\n",
    "    \n",
    "# Epanechnikov Kernel\n",
    "def Epanechnikov(x):\n",
    "  return np.where(np.abs(x)>1,0,3/4*(1-np.abs(x)**2))\n",
    "    \n",
    "# Quartic Kernel\n",
    "def Quartic(x):\n",
    "  return np.where(np.abs(x)>1,0,15/16*(1-np.abs(x)**2)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c5498b-c923-405e-9d16-7eea03138e55",
   "metadata": {},
   "source": [
    "## Part 1\n",
    "\n",
    "Create your class that implements the Gradient Boosting concept, based on the locally weighted regression method (Lowess class), and that allows a user-prescribed number of boosting steps. The class you develop should have all the mainstream useful options, including “fit,” “is_fitted”, and “predict,” methods. Show applications with real data for regression, 10-fold cross-validations and compare the effect of different scalers, such as the “StandardScaler”, “MinMaxScaler”, and the “QuantileScaler”. In the case of the “Concrete” data set, determine a choice of hyperparameters that yield lower MSEs for your method when compared to the eXtream Gradient Boosting library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b425966a-6716-4741-ad29-2c50713e22a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data = pd.read_csv('../data/concrete.csv')\n",
    "data = pd.read_csv('./data/concrete.csv')\n",
    "\n",
    "X = data.drop(columns='strength').values\n",
    "y = data['strength'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7cb9e363",
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_fold_cross_validation(model):\n",
    "    results = {}\n",
    "\n",
    "    # list of scalers to iterate through\n",
    "    scalers = {\n",
    "    \"StandardScaler\": StandardScaler(),\n",
    "    \"MinMaxScaler\": MinMaxScaler(),\n",
    "    \"QuantileScaler\": QuantileTransformer(n_quantiles=min(100, X.shape[0]), output_distribution='normal')\n",
    "    }\n",
    "\n",
    "    # initializing kfold cross-validation\n",
    "    kf = KFold(n_splits=10, shuffle=True)\n",
    "\n",
    "    for scaler_name, scaler in scalers.items():\n",
    "        # scaling features using the current scaler\n",
    "        X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "        # making list to store mse for each fold\n",
    "        mse_scores = []\n",
    "\n",
    "   \n",
    "        for train_index, test_index in kf.split(X_scaled):\n",
    "            # splitting into testing and training data\n",
    "            X_train, X_test = X_scaled[train_index], X_scaled[test_index]\n",
    "            y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "            # fitting model on train data\n",
    "            model.fit(X_train, y_train)\n",
    "\n",
    "            # predicing on test data\n",
    "            y_pred = model.predict(X_test)\n",
    "            \n",
    "            # calculate and append mse\n",
    "            mse = mean_squared_error(y_test, y_pred)\n",
    "            mse_scores.append(mse)\n",
    "\n",
    "        # calculate the mean of the mse for each scaler\n",
    "        results[scaler_name] = np.mean(mse_scores)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e9335db-e2c9-40cf-8bf0-5484851f0ce0",
   "metadata": {},
   "source": [
    "#### Our own gradient boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "57c807e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "lowess_model = Lowess.Lowess(kernel=Gaussian)\n",
    "ridge_model = Lowess.Lowess(kernel=Gaussian)\n",
    "booster = g_boost.GradientBooster(lowess_model, ridge_model, boosting_steps=3)  # calling booster class with combines two models and performing three boosting steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a535e106",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'StandardScaler': 53.9334824408945,\n",
       " 'MinMaxScaler': 253.39539809478447,\n",
       " 'QuantileScaler': 31.218286982812863}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# performing k fold validation using function above using booster model\n",
    "results = k_fold_cross_validation(booster)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f34316e5-7588-4006-8091-cf133fb628b6",
   "metadata": {},
   "source": [
    "#### eXtream Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2186dc1b-e18b-4912-92c9-eddf5c16eeb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgboost_model = xgb.XGBRegressor()  # creating xg boost regressor model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca56474e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# performing k fold validation using function above using xg boost model\n",
    "results_xgb = k_fold_cross_validation(xgboost_model)\n",
    "results_xgb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "734572b0-6982-48ff-9774-94af731366fa",
   "metadata": {},
   "source": [
    "### Key Findings\n",
    "\n",
    "Our lowest MSE for our own gradient boosting locally weighted regression method was using the Quantile Scaler with an MSE of 29.11. We used 3 boosting steps meaning the model has gone through three iterations of adding new decision trees to improve its predictions. We weren't able to reduce the MSE of own method below the eXtreme Gradient Boosting model with had the lowest MSE of 16.60 using the Quantile Scaler (like our method). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fee159c-ac59-4f8c-81f1-c0824671c63f",
   "metadata": {},
   "source": [
    "## Part 2\n",
    "Implement your own version of Locally Weighted Logistic Regression and compare its performance on the Iris data set with the version presented in this article: https://calvintchi.github.io/classical_machine_learning/2020/08/16/lwlr.html."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff729899-afc7-4b6a-9864-4714a6a08219",
   "metadata": {},
   "source": [
    "### Importing iris data, TTS, and Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f5694c12-d799-43b6-bfff-b836287c3575",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing data from sklearn\n",
    "iris = datasets.load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ead589ab-1c65-47fc-9e69-20de8388495b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining x and y\n",
    "X = iris.data\n",
    "y = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "abf44e5d-b132-4103-9603-1a9f9c3f784f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# performing tts\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2699fcfe-c6ef-4760-9812-b28537369e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaling data using MinMax Scaler, can also use other scalers which are currently commented out\n",
    "scaler = MinMaxScaler()\n",
    "#scaler = StandardScaler()\n",
    "#scaler = QuantileTransformer()\n",
    "\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec234f1-a491-49d2-91df-7b3dd6d03ce3",
   "metadata": {},
   "source": [
    "### Model Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e7a48034-4153-494e-ac61-f6e732b93bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the model with Gaussian kernel\n",
    "model = LowessLR.LocallyWeightedLogisticRegression(kernel=Gaussian, lr=0.01, tau=0.2)\n",
    "\n",
    "#model = LowessLR.LocallyWeightedLogisticRegression(kernel=Tricubic, lr=0.01, tau=0.05) \n",
    "#model = LowessLR.LocallyWeightedLogisticRegression(kernel=Epanechnikov, lr=0.01, tau=0.05) \n",
    "#model = LowessLR.LocallyWeightedLogisticRegression(kernel=Quartic, lr=0.01, tau=0.05) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "495ff138-bcde-4d06-8c57-95e622fb0d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fitting model to training data\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "24e5f329-e880-48b2-9b25-8c1b76fed6c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using trained model to make predictions on test data\n",
    "predictions = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b056ed66-0e37-485b-81fe-574912b36411",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating accuracy of predictions compared to the true labels (y_test) and printing results\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(f'Accuracy: {accuracy * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "526294cf-335d-4df3-a0f1-328896ed9f50",
   "metadata": {},
   "source": [
    "### Comparing results to Calvin Chi Paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b0453540-6523-4085-9f50-85f3c81a31d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# version presented in Calvin Chi article\n",
    "\n",
    "class locally_weighted_logistic_regression(object):\n",
    "    \n",
    "    def __init__(self, tau, reg = 0.0001, threshold = 1e-6):\n",
    "        self.reg = reg\n",
    "        self.threshold = threshold\n",
    "        self.tau = tau\n",
    "        self.w = None\n",
    "        self.theta = None\n",
    "        self.x = None\n",
    "\n",
    "    def weights(self, x_train, x):\n",
    "        sq_diff = (x_train - x)**2\n",
    "        norm_sq = sq_diff.sum(axis = 1)\n",
    "        return np.ravel(np.exp(- norm_sq / (2 * self.tau**2)))\n",
    "\n",
    "    def logistic(self, x_train):\n",
    "        return np.ravel(1 / (1 + np.exp(-x_train.dot(self.theta))))\n",
    "\n",
    "    def train(self, x_train, y_train, x):\n",
    "        self.w = self.weights(x_train, x)\n",
    "        self.theta = np.zeros(x_train.shape[1])\n",
    "        self.x = x\n",
    "        gradient = np.ones(x_train.shape[1]) * np.inf\n",
    "        while np.linalg.norm(gradient) > self.threshold:\n",
    "            # compute gradient\n",
    "            h = self.logistic(x_train)\n",
    "            gradient = x_train.T.dot(self.w * (np.ravel(y_train) - h)) - self.reg * self.theta\n",
    "            # Compute Hessian\n",
    "            D = np.diag(-(self.w * h * (1 - h)))\n",
    "            H = x_train.T.dot(D).dot(x_train) - self.reg * np.identity(x_train.shape[1])\n",
    "            # weight update\n",
    "            self.theta = self.theta - np.linalg.inv(H).dot(gradient)\n",
    "    \n",
    "    def predict(self,x):  # adjusted slightly to allow for input feature\n",
    "        return np.array(self.logistic(x) > 0.5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f7e37f37-b523-4b57-8945-90f48807ee11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training one v rest models\n",
    "model_dict = {}  # dictionary to store models\n",
    "\n",
    "# training a model for each class\n",
    "for cls in np.unique(y_train):  # iterating over each unique class label in the training data\n",
    "    binary_y_train = (y_train == cls).astype(int)  # creating binary target variable for current class\n",
    "    model = locally_weighted_logistic_regression(tau=.02)  # initializing model\n",
    "    model.train(X_train, binary_y_train, X_train)  # training  model\n",
    "    model_dict[cls] = model  # storing model for given class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e053713f-6db9-4fb9-94ab-854a665f8b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# making predictions\n",
    "predictions = []  # initializing list to store predictions\n",
    "\n",
    "for x_test in X_test: # iterating over each test sample\n",
    "    class_probs = [] # initializing list to store probabilities for each class\n",
    "    \n",
    "    for cls, model in model_dict.items():  # iterating over the items in model_dict\n",
    "        prob = model.predict(x_test.reshape(1, -1))  # calling predict method of current model to get prediction\n",
    "        class_probs.append(prob[0])  # appends predicted probability for current class to class_probs list\n",
    "    \n",
    "    # after evaluating all classes for current test sample, determine class with highest probability\n",
    "    predictions.append(np.argmax(class_probs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa3bd021-1249-434a-9371-4731724e0e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating accuracy of predictions compared to the true labels (y_test) and printing results\n",
    "\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(f'Accuracy: {accuracy * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8148a311-dd2d-4332-80f3-9df16bd1dc3e",
   "metadata": {},
   "source": [
    "### Key Findings\n",
    "\n",
    "Our Locally Weighted Logistic Regression model tends to perform better on the iris dataset than the model Calvin Chi presented. The accuracy rates are both quite variable, but our model generally performs better. We did choose to use the One vs Many approach of classification with both models. Since in the iris dataset there are three classifications, three separate logistic regression models are created. So these three models would be:\n",
    "1. Setosa vs Not setosa\n",
    "2. Versicolour vs Not Versicolour\n",
    "3. Virginia vs Not Virginia\n",
    "\n",
    "Each model outputs a probability that that instance belongs in the class. The final class prediction is then determined to be the one with the highest probability of the three models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d15921-45ee-4f2c-894d-3690fcaa93ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
