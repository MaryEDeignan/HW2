{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41c8e572-9ceb-4153-8695-f67a9938ecc6",
   "metadata": {},
   "source": [
    "# HW 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a46530f-25fa-4e6f-8796-0c6c852ee674",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1a79511-28e9-4419-8061-5da719c99833",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.preprocessing import MinMaxScaler,StandardScaler,QuantileTransformer\n",
    "from scipy.spatial.distance import cdist\n",
    "from sklearn import linear_model, datasets\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c812e073-88f9-4b26-84b8-fe7908f6e221",
   "metadata": {},
   "outputs": [],
   "source": [
    "import Lowess\n",
    "import Logistic as LowessLR\n",
    "import g_boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d1690bd-ff35-434b-9829-66aef4102f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install xgboost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da631247-68aa-43c2-b118-6b36c17f0c8d",
   "metadata": {},
   "source": [
    "#### Kernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "de8bf85f-6e9c-4163-80a8-f0e096f20bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining Kernels\n",
    "\n",
    "# Gaussian Kernel\n",
    "def Gaussian(x):\n",
    "  return np.where(np.abs(x)>4,0,1/(np.sqrt(2*np.pi))*np.exp(-1/2*x**2))\n",
    "    \n",
    "# Tricubic Kernel\n",
    "def Tricubic(x):\n",
    "  return np.where(np.abs(x)>1,0,(1-np.abs(x)**3)**3)\n",
    "    \n",
    "# Epanechnikov Kernel\n",
    "def Epanechnikov(x):\n",
    "  return np.where(np.abs(x)>1,0,3/4*(1-np.abs(x)**2))\n",
    "    \n",
    "# Quartic Kernel\n",
    "def Quartic(x):\n",
    "  return np.where(np.abs(x)>1,0,15/16*(1-np.abs(x)**2)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c5498b-c923-405e-9d16-7eea03138e55",
   "metadata": {},
   "source": [
    "## Part 1\n",
    "\n",
    "Create your class that implements the Gradient Boosting concept, based on the locally weighted regression method (Lowess class), and that allows a user-prescribed number of boosting steps. The class you develop should have all the mainstream useful options, including “fit,” “is_fitted”, and “predict,” methods. Show applications with real data for regression, 10-fold cross-validations and compare the effect of different scalers, such as the “StandardScaler”, “MinMaxScaler”, and the “QuantileScaler”. In the case of the “Concrete” data set, determine a choice of hyperparameters that yield lower MSEs for your method when compared to the eXtream Gradient Boosting library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b425966a-6716-4741-ad29-2c50713e22a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data = pd.read_csv('../data/concrete.csv')\n",
    "data = pd.read_csv('./data/concrete.csv')\n",
    "\n",
    "X = data.drop(columns='strength').values\n",
    "y = data['strength'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7a58ac05-56f3-40a7-8646-ceabce066d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scalers to evaluate\n",
    "scalers = {\n",
    "    \"StandardScaler\": StandardScaler(),\n",
    "    \"MinMaxScaler\": MinMaxScaler(),\n",
    "    \"QuantileScaler\": QuantileTransformer(n_quantiles=min(900, X.shape[0]), output_distribution='normal')\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "67c1a998-b25f-4cc5-a461-184203d4a79e",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "\n",
    "for scaler_name, scaler in scalers.items():\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "    # Initialize 10-fold cross-validation\n",
    "    kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "    \n",
    "    mse_scores = []\n",
    "\n",
    "    # Perform cross-validation\n",
    "    for train_index, test_index in kf.split(X_scaled):\n",
    "        X_train, X_test = X_scaled[train_index], X_scaled[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        \n",
    "        # Instantiate models\n",
    "        lowess_model = Lowess.Lowess(kernel=Gaussian)\n",
    "        ridge_model = Ridge(alpha=0.001)\n",
    "        booster = g_boost.GradientBooster(lowess_model, ridge_model, boosting_steps=10)\n",
    "        \n",
    "        # Fit the model\n",
    "        booster.fit(X_train, y_train)\n",
    "        \n",
    "        # Predict and evaluate\n",
    "        y_pred = booster.predict(X_test)\n",
    "        mse = mean_squared_error(y_test, y_pred)\n",
    "        mse_scores.append(mse)\n",
    "\n",
    "    # Store the mean MSE for the current scaler\n",
    "    results[scaler_name] = np.mean(mse_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3b7f2def-c35d-4eb9-87f8-e6abc7d9bd8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StandardScaler: MSE = 860.2757\n",
      "MinMaxScaler: MSE = 101.7934\n",
      "QuantileScaler: MSE = 951.1050\n"
     ]
    }
   ],
   "source": [
    "for scaler_name, mse_value in results.items():\n",
    "    print(f\"{scaler_name}: MSE = {mse_value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db8e2e6-0e33-45d1-8fbf-9586e96ac4b0",
   "metadata": {},
   "source": [
    "#### eXtreme Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1c3702a7-9e6b-4428-8559-bdcfaf3e2260",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost with StandardScaler: Mean MSE = 111.2413\n",
      "XGBoost with MinMaxScaler: Mean MSE = 111.2413\n",
      "XGBoost with QuantileScaler: Mean MSE = 111.2413\n"
     ]
    }
   ],
   "source": [
    "# initializing list to store results of XGBoost\n",
    "xgboost_results = []\n",
    "\n",
    "# looping through each scaler in scaler dictionary from first part\n",
    "for scaler_name, scaler in scalers.items():\n",
    "    # initializing list to store MSE scores for each fold in cross-validation\n",
    "    mse_scores = []\n",
    "\n",
    "    # performing k-fold cross-validation\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        # splitting the dataset into training and testing sets based on indices\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "        # scaling training data based on current scaler\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        # scaling testing data using the same scaler\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "        \n",
    "        # creating DMatrix for XGBoost (data structure optimized for XGBoost)\n",
    "        dtrain = xgb.DMatrix(X_train_scaled, label=y_train)\n",
    "        dtest = xgb.DMatrix(X_test_scaled, label=y_test)\n",
    "\n",
    "        # setting parameters for XGBoost model\n",
    "        params = {\n",
    "            'objective': 'reg:squarederror', # objective function for regression\n",
    "            'max_depth': 3,    # maximum tree depth, maximum number of levels a tree can have\n",
    "            'learning_rate': 0.001,  # learning rate\n",
    "        }\n",
    "\n",
    "        # training the XGBoost model with the specified parameters\n",
    "        xgboost_model = xgb.train(params, dtrain, num_boost_round=1000)\n",
    "\n",
    "        # making predictions on test set\n",
    "        y_pred_xgb = xgboost_model.predict(dtest)\n",
    "\n",
    "        # calculating mse for predictions\n",
    "        mse_xgb = mean_squared_error(y_test, y_pred_xgb)\n",
    "        # adding MSE score to the list of scores\n",
    "        mse_scores.append(mse_xgb)\n",
    "\n",
    "    # adding the mean MSE score for this scaler to the results list\n",
    "    xgboost_results.append(np.mean(mse_scores))\n",
    "\n",
    "# displaying the average MSE results for each scaler\n",
    "for scaler_name, mse in zip(scalers.keys(), xgboost_results):\n",
    "    print(f\"XGBoost with {scaler_name}: Mean MSE = {mse:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fee159c-ac59-4f8c-81f1-c0824671c63f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Part 2\n",
    "Implement your own version of Locally Weighted Logistic Regression and compare its performance on the Iris data set with the version presented in this article: https://calvintchi.github.io/classical_machine_learning/2020/08/16/lwlr.html."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff729899-afc7-4b6a-9864-4714a6a08219",
   "metadata": {},
   "source": [
    "### Importing iris data, TTS, and Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f5694c12-d799-43b6-bfff-b836287c3575",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing data from sklearn\n",
    "iris = datasets.load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ead589ab-1c65-47fc-9e69-20de8388495b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining x and y variables\n",
    "X = iris.data\n",
    "y = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "abf44e5d-b132-4103-9603-1a9f9c3f784f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# performing test-train split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2699fcfe-c6ef-4760-9812-b28537369e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaling data using MinMax Scaler, can also use other scalers which are currently commented out\n",
    "scaler = MinMaxScaler()\n",
    "#scaler = StandardScaler()\n",
    "#scaler = QuantileTransformer()\n",
    "\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec234f1-a491-49d2-91df-7b3dd6d03ce3",
   "metadata": {},
   "source": [
    "### Model Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e7a48034-4153-494e-ac61-f6e732b93bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a LocallyWeightedLogisticRegression model with Gaussian kernel, learing rate of  0.01, and tau=0.05\n",
    "model = LowessLR.LocallyWeightedLogisticRegression(kernel=Gaussian, lr=0.01, tau=0.05) # accuracy of .97\n",
    "\n",
    "#model = LowessLR.LocallyWeightedLogisticRegression(kernel=Tricubic, lr=0.01, tau=0.05) # accuracy of .97\n",
    "#model = LowessLR.LocallyWeightedLogisticRegression(kernel=Epanechnikov, lr=0.01, tau=0.05) # accuracy of .97\n",
    "#model = LowessLR.LocallyWeightedLogisticRegression(kernel=Quartic, lr=0.01, tau=0.05) # accuracy of .97"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "495ff138-bcde-4d06-8c57-95e622fb0d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fitting model to training data\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "24e5f329-e880-48b2-9b25-8c1b76fed6c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using trained model to make predictions on test data\n",
    "predictions = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b056ed66-0e37-485b-81fe-574912b36411",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 96.67%\n"
     ]
    }
   ],
   "source": [
    "# Calculating accuracy of predictions compared to the true labels (y_test) and printing results\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(f'Accuracy: {accuracy * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "526294cf-335d-4df3-a0f1-328896ed9f50",
   "metadata": {},
   "source": [
    "### Comparing results to Calvin Chi Paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b0453540-6523-4085-9f50-85f3c81a31d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# version presented in Calvin Chi article\n",
    "\n",
    "class locally_weighted_logistic_regression(object):\n",
    "    \n",
    "    def __init__(self, tau, reg = 0.0001, threshold = 1e-6):\n",
    "        self.reg = reg\n",
    "        self.threshold = threshold\n",
    "        self.tau = tau\n",
    "        self.w = None\n",
    "        self.theta = None\n",
    "        self.x = None\n",
    "\n",
    "    def weights(self, x_train, x):\n",
    "        sq_diff = (x_train - x)**2\n",
    "        norm_sq = sq_diff.sum(axis = 1)\n",
    "        return np.ravel(np.exp(- norm_sq / (2 * self.tau**2)))\n",
    "\n",
    "    def logistic(self, x_train):\n",
    "        return np.ravel(1 / (1 + np.exp(-x_train.dot(self.theta))))\n",
    "\n",
    "    def train(self, x_train, y_train, x):\n",
    "        self.w = self.weights(x_train, x)\n",
    "        self.theta = np.zeros(x_train.shape[1])\n",
    "        self.x = x\n",
    "        gradient = np.ones(x_train.shape[1]) * np.inf\n",
    "        while np.linalg.norm(gradient) > self.threshold:\n",
    "            # compute gradient\n",
    "            h = self.logistic(x_train)\n",
    "            gradient = x_train.T.dot(self.w * (np.ravel(y_train) - h)) - self.reg * self.theta\n",
    "            # Compute Hessian\n",
    "            D = np.diag(-(self.w * h * (1 - h)))\n",
    "            H = x_train.T.dot(D).dot(x_train) - self.reg * np.identity(x_train.shape[1])\n",
    "            # weight update\n",
    "            self.theta = self.theta - np.linalg.inv(H).dot(gradient)\n",
    "    \n",
    "    def predict(self,x):  # adjusted slightly to allow for input feature\n",
    "        return np.array(self.logistic(x) > 0.5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f7e37f37-b523-4b57-8945-90f48807ee11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training one v rest models\n",
    "model_dict = {}  # initialize dictionary to store models\n",
    "\n",
    "# training a model for each class\n",
    "for cls in np.unique(y_train):  # iterating over each unique class label in the training data\n",
    "    binary_y_train = (y_train == cls).astype(int)  # creating binary target variable for current class\n",
    "    model = locally_weighted_logistic_regression(tau=.05)  # initializing model\n",
    "    model.train(X_train, binary_y_train, X_train)  # training  model\n",
    "    model_dict[cls] = model  # storing model for given class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e053713f-6db9-4fb9-94ab-854a665f8b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# making predictions\n",
    "predictions = []  # initializing list to store predictions\n",
    "\n",
    "for x_test in X_test: # iterating over each test sample\n",
    "    class_probs = [] # initializing list to store probabilities for each class\n",
    "    \n",
    "    for cls, model in model_dict.items():  # iterating over the items in model_dict\n",
    "        prob = model.predict(x_test.reshape(1, -1))  # calling predict method of current model to get prediction\n",
    "        class_probs.append(prob[0])  # appends predicted probability for current class to class_probs list\n",
    "    \n",
    "    # after evaluating all classes for current test sample, determine class with highest probability\n",
    "    predictions.append(np.argmax(class_probs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fa3bd021-1249-434a-9371-4731724e0e69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 83.33%\n"
     ]
    }
   ],
   "source": [
    "# Calculating accuracy of predictions compared to the true labels (y_test) and printing results\n",
    "\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(f'Accuracy: {accuracy * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5837626a-155d-43b8-9f25-5a07b737d658",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c621fb-b4d0-4ab0-b99e-7ea737a94caa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
