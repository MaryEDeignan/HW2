<!doctype html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="color-scheme" content="light dark">
        <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@picocss/pico@2/css/pico.min.css">

        <title>Assignment 2 part 1</title>
    </head>

    <main class="container">
        <h1>Gradient Boosting</h1>

        <h3>Introduction</h3>
        <p>Gradient boosting involves repeatedly training a model on its own residuals. 
            Over time, the model (considered a weak learner) improves its predictions, because it is training on both the train data and the results of previous predictions.
            First, the model is fit to the training data. Then, the model's predictions are subtracted from the actual predictor values.
            A second model is then fit to the training data and the residuals. This process is repeated for n steps. Finally, the model predicts the test data.</p>
        
        <img src="../boosting_diagram.png" alt="Diagram of the iterative boosting process as explained in the first paragraph.">

        <h3>Process</h3>
        <ol>
            <li>
                We created the <code>GradientBooster</code> class. This class takes two models: the first model, trained on just the training data, and the residuals model, trained on the residuals of the first model.
                    For n boosting steps, the GradientBooster class will fit models, predict based on the training data, and calculate the residuals. When called to predict data, the class will use its most recently trained model to predict the test data.
            </li>
            <li>
                The GradientBooster class acts as a model, so we used it alongside k-fold cross-validation to predict concrete strength. We evaluated our model with mean squared error, using three different scalers.
            </li>
            <li>
                We implemented the <code>xgboost</code> package to act as a comparison. We used mean squared error and k-fold cross-validation to measure the effectiveness of the package.
            </li>
        </ol>

        <h3>Conclusions</h3>
        <p>
            We were able to create a gradient boosting model that iterates over a given number of boosting steps to fit a model. This model predicts new data based on its previously trained data.
            Our best MSE for our gradient booster with two lowess models was about 32, using a quantile scaler. XGBoost was still the better model using a quantile scaler, with a MSE of about 16.
            We used 3 gradient boosting steps to accomplish this.
        </p>

        <p><a href="index.html">Go Back</a></p>
    </main>
</html>